{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space model\n",
    "\n",
    "---\n",
    "**Author**: Marko Bajec\n",
    "\n",
    "**Last update**: 13.4.2019\n",
    "\n",
    "**Description**: in this example we show how to calculate TF-IDF based document representations.  \n",
    "\n",
    "**Required libraries** (use pip3):\n",
    "* TBA\n",
    "\n",
    "---\n",
    "## Document corpus\n",
    "Let's say we have the following 5 sentences representing the **document corpus**:\n",
    "* d<sub>1</sub>: *Remeo and Juliet.*\n",
    "* d<sub>2</sub>: *Juliet: O happy dagger!*\n",
    "* d<sub>3</sub>: *Romeo died by dagger.*\n",
    "* d<sub>4</sub>: *\"Live free or die”, that’s the New-Hampshire’s motto.*\n",
    "* d<sub>5</sub>: *Did you know, New-Hampshire is in New-England.*\n",
    "\n",
    "**Question**: How close (relevant) are the above sentences to the following query: $q=$<span style=\"color:blue\">*died, dagger*</span>?\n",
    "\n",
    "## Python implementation\n",
    "In the Python program below we calculate how relevant is each of the documents $d_i$ in the corpus to the given query $q$ by using different relevance measures:\n",
    "* Dot product\n",
    "* Cosine distance\n",
    "* Ocapi method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import required libraries \n",
    "Apart from importing the required libraries, two methods are defined in this code snippet, <code>get_word_pos</code> and <code>doc_num</code>, both requited later on in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import numpy\n",
    "import math\n",
    "from collections import Counter\n",
    "from texttable import Texttable\n",
    "\n",
    "# lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        #if not J, V, N or R then make it default, i.e. \"n\" as naun \n",
    "        return 'n'\n",
    "\n",
    "# returns document id (either di or q)\n",
    "def doc_num(i):\n",
    "    if i == len(corpus):\n",
    "        return \"q\"\n",
    "    else:\n",
    "        return \"d\" + str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define corpus and query\n",
    "Notice that query $q$ is treated as the last document in the **corpus**. This is to assure that the same transformations will be done to $q$ as to other documents in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document corpus\n",
    "corpus = [\"Romeo and Juliet.\", \n",
    "          \"Juliet: O happy dagger!\", \n",
    "          \"Romeo died by dagger.\", \n",
    "          \"'Live free or die'”, that’s the New-Hampshire’s motto.\", \n",
    "          \"Did you know, New-Hampshire is in New-England.\"\n",
    "         ]\n",
    "\n",
    "# query\n",
    "query = \"died, dagger\"\n",
    "\n",
    "# query will be treated as last document of the corpus\n",
    "corpus = numpy.append(corpus, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Processing corpus documents\n",
    "In this step, several transformations are performed on the corpus documents:\n",
    "* documents are tokenized\n",
    "* all letters are changed to lowercase\n",
    "* punctations and stopwords are removed\n",
    "* tokens are lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "text = []        \n",
    "for i in range(0, len(corpus)):\n",
    "    text.extend(nltk.word_tokenize(corpus[i]))\n",
    "\n",
    "# change words to lowercase\n",
    "text = [x.lower() for x in text]\n",
    "\n",
    "# remove punctation\n",
    "nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit \n",
    "text_filtered = [w for w in text if nonPunct.match(w)]\n",
    "\n",
    "# remove stopwords\n",
    "text_filtered = [word for word in text_filtered if word not in stopwords.words('english')]\n",
    "\n",
    "# lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordsPOS = nltk.pos_tag(text_filtered)\n",
    "wordsPOSset = {}\n",
    "for i in range (0, len(wordsPOS)):\n",
    "    wordsPOSset.update({(wordsPOS[i][0]):wordsPOS[i][1][0]})\n",
    "\n",
    "text_lemmatized = []\n",
    "for word in text_filtered:\n",
    "    text_lemmatized.append(lemmatizer.lemmatize(word, pos=get_wordnet_pos(wordsPOSset[word])))\n",
    "    \n",
    "print(\"Tokens without punctation and stopwords\")\n",
    "print(text_filtered)\n",
    "print(\" \")\n",
    "print(\"Lemmatized tokens, vocabulary\")\n",
    "print(set(text_lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create TF matrix\n",
    "Three matrices are created:\n",
    "* TF matrix,\n",
    "* normalized TF matrix (using Euclidean normalization) and\n",
    "* TF-IDF matrix.\n",
    "\n",
    "#### Euclidean normalization\n",
    "<span style=\"color:darkred\">\n",
    "$$\\large tf_{ij} = \\frac{f_{ij}}{\\sqrt{f_{1j}^2+f_{2j}^2+ ... + f_{|V|j}^2}}$$\n",
    "</span>\n",
    "\n",
    "#### TF-IDF\n",
    "<span style=\"color:darkred\">\n",
    "$$\\large w_{ij} = tf_{ij}\\cdot idf_{i} = \\frac{f_{ij}}{\\sqrt{f_{1j}^2+f_{2j}^2+ ... + f_{|V|j}^2}}\\cdot log \\frac{N}{df_i}$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words counts\n",
    "text_counts = Counter(text_lemmatized)\n",
    "dist_words = set(text_counts)\n",
    "num_dist_words = len(dist_words)\n",
    "corpus_length = len(corpus)\n",
    "\n",
    "# create TF matrix\n",
    "tf = numpy.zeros((corpus_length, num_dist_words))\n",
    "j = -1\n",
    "\n",
    "for word in dist_words:\n",
    "    #print(word)\n",
    "    j = j + 1\n",
    "    #print(j)\n",
    "    for i in range(0, corpus_length):\n",
    "        if word in corpus[i].lower():\n",
    "            tf[i][j] = tf[i][j] + 1\n",
    "            \n",
    "# normalize TF matrix with Euclidean normalization\n",
    "tf_norm = numpy.zeros((len(corpus), num_dist_words))\n",
    "sqr = numpy.zeros(corpus_length)\n",
    "for i in range(0, corpus_length):\n",
    "    sqr[i] = 0\n",
    "    for j in range(0, num_dist_words):\n",
    "        sqr[i] += math.pow(tf[i][j], 2)\n",
    "    sqr[i] = math.sqrt(sqr[i])\n",
    "\n",
    "for i in range(0, corpus_length):\n",
    "    for j in range(0, num_dist_words):\n",
    "        tf_norm[i][j] = tf[i][j]/sqr[i]\n",
    "        \n",
    "# create TF-IDF matrix\n",
    "idf = numpy.zeros(num_dist_words)\n",
    "for i in range(0, num_dist_words):\n",
    "    idf[i] = 0\n",
    "    for j in range(0, corpus_length):\n",
    "        idf[i] += tf[j][i]\n",
    "\n",
    "tfidf = numpy.zeros((corpus_length, num_dist_words))\n",
    "for i in range(0, corpus_length):\n",
    "    for j in range(0, num_dist_words):\n",
    "        tfidf[i][j] = tf_norm[i][j] * math.log(corpus_length/idf[j])\n",
    "\n",
    "# print results\n",
    "\n",
    "table = Texttable(0)\n",
    "table.set_cols_align(numpy.append([\"l\"], numpy.full((1, num_dist_words), \"r\")[0]))\n",
    "table.set_cols_valign(numpy.full((1, num_dist_words+1), \"m\")[0])\n",
    "\n",
    "print(\"TF matrix\")\n",
    "table.header(numpy.append([\"document\"], list(dist_words)))\n",
    "for i in range(0, corpus_length):\n",
    "    table.add_row(numpy.append([doc_num(i+1)],tf[i]))\n",
    "    \n",
    "print(table.draw() + \"\\n\")\n",
    "\n",
    "print(\"Normalized TF matrix\")\n",
    "table.reset()\n",
    "table.header(numpy.append([\"document\"], list(dist_words)))\n",
    "for i in range(0, corpus_length):\n",
    "    table.add_row(numpy.append([doc_num(i+1)],tf_norm[i]))\n",
    "    \n",
    "print(table.draw() + \"\\n\")\n",
    "\n",
    "print(\"TF-IDF matrix\")\n",
    "table.reset()\n",
    "table.header(numpy.append([\"document\"], list(dist_words)))\n",
    "for i in range(0, corpus_length):\n",
    "    table.add_row(numpy.append([doc_num(i+1)],tfidf[i]))\n",
    "    \n",
    "print(table.draw() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculating relevance of documents to the query\n",
    "Relevance can be calculated using different measures. Here we use:\n",
    "* Dot product and\n",
    "* Cosine similarity or distance.\n",
    "\n",
    "#### Dot product\n",
    "<span style=\"color:darkred\">\n",
    "$$\\large sim(\\mathbf{d}_j,\\mathbf{q}) = \\big \\langle \\mathbf{d}_j \\cdot \\mathbf{q} \\big \\rangle$$\n",
    "</span>\n",
    "\n",
    "#### Cosine similarity\n",
    "<span style=\"color:darkred\">\n",
    "$$\\large cosine(\\mathbf{d}_j,\\mathbf{q}) = \\frac{\\big \\langle \\mathbf{d}_j \\cdot \\mathbf{q} \\big \\rangle} \n",
    "{\\Bigl\\| \\begin{matrix} \\mathbf{d}_j \\end{matrix} \\Bigr\\| \\cdot \n",
    "\\Bigl\\| \\begin{matrix} \\mathbf{q} \\end{matrix} \\Bigr\\|} =\n",
    "\\frac{\\sum_{i=1}^{|V|}{w_{ij} \\cdot w_{iq}}}\n",
    "{\\sqrt{\\sum_{i=1}^{|V|}{w_{ij}^2}} \\cdot \\sqrt{\\sum_{i=1}^{|V|}{w_{iq}^2}}}\n",
    "$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate relevance of di to q\n",
    "\n",
    "# A) using dot product (di * q)\n",
    "dot_product = numpy.zeros(corpus_length-1)\n",
    "for i in range(0, corpus_length - 1):\n",
    "    dot_product[i] = numpy.dot(tfidf[i], tfidf[corpus_length-1])\n",
    "   \n",
    "# B) using cosine distance\n",
    "cosine_distance = numpy.zeros(corpus_length-1)\n",
    "for i in range(0, corpus_length - 1):\n",
    "    cosine_distance[i] = dot_product[i]/(numpy.linalg.norm(tfidf[i]) * numpy.linalg.norm(tfidf[corpus_length-1]))\n",
    "    \n",
    "# C) using Okapi distance\n",
    "cosine_distance = numpy.zeros(corpus_length-1)\n",
    "for i in range(0, corpus_length - 1):\n",
    "    cosine_distance[i] = dot_product[i]/(numpy.linalg.norm(tfidf[i]) * numpy.linalg.norm(tfidf[corpus_length-1]))\n",
    " \n",
    "print(\"Document relevance based on dot product\")\n",
    "table1 = Texttable(0)\n",
    "table1.set_cols_align([\"l\", \"r\", \"r\"])\n",
    "table1.set_cols_valign([\"m\", \"m\", \"m\"])\n",
    "\n",
    "table1.header([\"Method\", \"Dot product\", \"Cosine similarity\"])\n",
    "for i in range(0, corpus_length-1):\n",
    "    table1.add_row([doc_num(i+1), dot_product[i], cosine_distance[i]])\n",
    "\n",
    "print(table1.draw() + \"\\n\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
