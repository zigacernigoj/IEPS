{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with Naive Bayes\n",
    "\n",
    "---\n",
    "**Author**: Marko Bajec\n",
    "\n",
    "**Last update**: 11.2.2019\n",
    "\n",
    "**Description**: in this example we show how to use Naive Bayes algorithm to classify a document into one of the predefined categories.  \n",
    "\n",
    "**Required libraries** (use pip3):\n",
    "* <code>nltk</code> \n",
    "* <code>re</code>\n",
    "* <code>texttable</code>\n",
    "* <code>counter</code>\n",
    "\n",
    "---\n",
    "## Naive Bayes\n",
    "**Naive Bayes** is a family of probabilistic algorithms. It is based on **Bayes' Theorem**, which describes the probability of a feature, based on prior knowledge of conditions that might be related to that feature. \n",
    "\n",
    "Let's say we have a *taxonomy* with the following two *categories*:\n",
    "* c<sub>1</sub>: <code>sport</code>, \n",
    "* c<sub>2</sub>: <code>politics</code>.\n",
    "\n",
    "In the *training dataset* we have five tagged documents:\n",
    "1. *A great game.* :<code>sports</code>,\n",
    "2. *The election was over.* : <code>politics</code>,\n",
    "3. *Very clean match.* : <code>sports</code>,\n",
    "4. *A clean but forgettable game.* : <code>sports</code>, and\n",
    "5. *It was a close election.* : <code>politics</code>.\n",
    "\n",
    "**Question**: Is the following text about <code>sports</code> or <code>politics</code>?\n",
    "<span style=\"color:blue\">*A very close game.*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python implementation\n",
    "Let's first get some **statistics** based on the training dataset. We will need to know:\n",
    "* what is the portion of documents in the training dataset that belong to each category,\n",
    "* how many distinct words we have in the dataset\n",
    "* what is the frequency of each word in documents of each category\n",
    "* what is the probability that a word $w$ appears in a document $d$ of a category $c$\n",
    "\n",
    "We will also need to employ **Laplace Smoothing** to toggle zero values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from texttable import Texttable\n",
    "\n",
    "# categories\n",
    "c1 = \"sports\"\n",
    "c2 = \"politics\"\n",
    "\n",
    "# training documents\n",
    "training_dataset = [[\"A great game.\", c1], [\"The election was over.\", c2], [\"Very clean match.\", c1], \n",
    "                    [\"A clean but forgettable game.\", c1], [\"It was a close election.\", c2]]\n",
    "\n",
    "# representation of each category in the training dataset\n",
    "Pc1 = 3/5\n",
    "Pc2 = 2/5\n",
    "\n",
    "# document to be classified\n",
    "sentance = \"A very close game\"\n",
    "\n",
    "# class to keep:\n",
    "# - distinct words (word),\n",
    "# - their frequencies for each category (fc1, fc2) \n",
    "# - their probability of occurence in each category (prwordc1, prwordc2)\n",
    "# - their probabilities using Laplace smoothing (lapc1, lapc2)\n",
    "class kword:\n",
    "    def __init__(self, word, fc1, fc2, prwordc1, prwordc2, lapc1, lapc2):\n",
    "        self.word = word\n",
    "        self.fc1 = fc1\n",
    "        self.fc2 = fc2\n",
    "        self.prwordc1 = prwordc1\n",
    "        self.prwordc2 = prwordc2\n",
    "        self.lapc1 = lapc1\n",
    "        self.lapc2 = lapc2\n",
    "\n",
    "# tokenize - separatelly for category c1 and c2\n",
    "text, text_c1, text_c2 = [], [], []        \n",
    "for i in range(0, len(training_dataset)):\n",
    "    text.extend(nltk.word_tokenize(training_dataset[i][0]))\n",
    "    if training_dataset[i][1] == c1:\n",
    "        text_c1.extend(nltk.word_tokenize(training_dataset[i][0]))\n",
    "    else:\n",
    "        text_c2.extend(nltk.word_tokenize(training_dataset[i][0]))\n",
    "\n",
    "# change words to lowercase\n",
    "text = [x.lower() for x in text]\n",
    "text_c1 = [x.lower() for x in text_c1]\n",
    "text_c2 = [x.lower() for x in text_c2]\n",
    "\n",
    "# remove punctation\n",
    "nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "text_filtered = [w for w in text if nonPunct.match(w)]\n",
    "text_c1_filtered = [w for w in text_c1 if nonPunct.match(w)]\n",
    "text_c2_filtered = [w for w in text_c2 if nonPunct.match(w)]\n",
    "\n",
    "text_counts = Counter(text_filtered)\n",
    "text_c1_counts = Counter(text_c1_filtered)\n",
    "text_c2_counts = Counter(text_c2_filtered)\n",
    "\n",
    "# get words counts\n",
    "dist_words = set(text_counts)\n",
    "num_dist_words = len(dist_words)\n",
    "num_words_c1 = len(text_c1_filtered)\n",
    "num_words_c2 = len(text_c2_filtered)\n",
    "\n",
    "# create result list\n",
    "# for each word, check how many times it appears in c1 and c2, calculate probability of a word in a category \n",
    "# and smooth the results using Laplace \n",
    "results = []\n",
    "for word in dist_words:\n",
    "    fc1 = text_c1_counts[word]\n",
    "    fc2 = text_c2_counts[word]\n",
    "    results.append(kword(word, fc1, fc2, fc1/num_words_c1, fc2/num_words_c2, \n",
    "                         (fc1+1)/(num_words_c1+num_dist_words), (fc2+1)/(num_words_c2+num_dist_words)))\n",
    "    \n",
    "# print results\n",
    "\n",
    "print(\"Number of documents in the training dataset: %s\" % len(training_dataset))\n",
    "print(\"Distribution of docs per category: c1 = %s, c2 = %s\" % (Pc1, Pc2))\n",
    "print(\"Number of distinct words: %s\" % num_dist_words)\n",
    "\n",
    "table = Texttable(0) #0 means the table width is unlimitted\n",
    "table.set_cols_align([\"l\", \"r\", \"r\", \"r\", \"r\", \"r\", \"r\"])\n",
    "table.set_cols_valign([\"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\"])\n",
    "#table.add_rows([[\"Word\", \"fc1\", \"fc2\", \"Pr(word|c1)\", \"Pr(word|c2)\", \"Lap(Pr(word|c1))\", \"Lap(Pr(word|c2))\"],\n",
    "#               [\"Test\", 32, 2.019, 0, 0, 0, 0]])\n",
    "table.header([\"Word\", \"fc1\", \"fc2\", \"Pr(word|c1)\", \"Pr(word|c2)\", \"Lap(Pr(word|c1))\", \"Lap(Pr(word|c2))\"])\n",
    "for row in results:\n",
    "    table.add_row([row.word, row.fc1, row.fc2, row.prwordc1, row.prwordc2, row.lapc1, row.lapc2])\n",
    "    \n",
    "print(table.draw() + \"\\n\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which category is more likely the observed document belongs to?  \n",
    "Let's now calculate the probability that the document \"*A very close geme*\" belongs to either of the categories, <code>sport</code> or <code>politics</code>. To do this we use the following formula:\n",
    "\n",
    "<span style=\"color:darkred\">\n",
    "$$\\LARGE ùëê_ùëó = \\underset{c_k \\in C}{\\operatorname{argmax}}Pr‚Å°(ùëê_ùëò)\\prod_{w=1}^{|V|} Pr(w_i|c_k)^{f_{w_{i}}}$$\n",
    "</span>\n",
    "\n",
    "Let's say our document $D$ that we would like to classify, consists of words $w_1, w_2,..., w_n \\in D$. The probability that $D$ belongs to the category $c$ is calculated as the probability of category $c$, i.e. $Pr(c)$ multiplied with the product of probabilities of occurencies of words $w_i$ in the observed document $D$, i.e. $Pr(w_i|c)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word list for the observed document\n",
    "target_doc = nltk.word_tokenize(sentance)\n",
    "target_doc = [x.lower() for x in target_doc]\n",
    "\n",
    "# calculate category probability\n",
    "Pr_wC1 = 1\n",
    "Pr_wC2 = 1\n",
    "\n",
    "for i in results:\n",
    "  if i.word in target_doc:\n",
    "        Pr_wC1 = Pr_wC1 * i.lapc1\n",
    "        Pr_wC2 = Pr_wC2 * i.lapc2\n",
    "\n",
    "print(\"Probability of category c1 is %.8f\" % round((Pc1 * Pr_wC1), 10))\n",
    "print(\"Probability of category c2 is %.8f\" % round((Pc2 * Pr_wC2), 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
