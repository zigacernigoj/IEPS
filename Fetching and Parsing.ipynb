{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching and parsing in Python\n",
    "\n",
    "---\n",
    "**Author**: Marko Bajec\n",
    "\n",
    "**Last update**: 20.2.2019\n",
    "\n",
    "**Description**: in this notebook I describe a selection of libraries that might come handy if for **fetching** and **parsing** pages as well as for **text analysis** including **natural language processing**. Many of these libraries are required in other notebooks that I share for this course. The libraries discussed are:\n",
    "* <code>urllib</code>: [urllib](https://docs.python.org/3/library/urllib.html#module-urllib) is a package of modules for working with URLs,\n",
    "* <code>beautifulsoup4</code>: [beautifulsoap](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a library for pulling data out of HTML and XML files. Works with DOM,\n",
    "* <code>nltk</code>: [nltk](https://www.nltk.org) is a natural language processing toolkit, \n",
    "* <code>polyglot</code>: [polyglot](https://polyglot.readthedocs.io/en/latest/) is another library for natural language processing which includes support for Slovenian language,\n",
    "* <code>re</code>: [re]() is a library for working with regular expressions\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import all modules at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import polyglot\n",
    "import re\n",
    "import string\n",
    "import urllib\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetching pages\n",
    "### Simple fetching\n",
    "We will first show how <code>urllib</code> can be used for fetching pages from the Web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = urllib.request.urlopen('http://www.times.si')\n",
    "print(f.read(1000).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = urllib.request.urlopen('http://www.delo.si')\n",
    "print(f.read(5000).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that simple fetching does not suffice for pulling out all the data if dynamic pages are used. To render a typical page, a browser would usually execute several additional HTTP requests to download all the required files. Moreover, modern web pages are dynamic, i.e. they use scripting languages to pull data from a database and then render it on the page. \n",
    "\n",
    "### Headless browsers\n",
    "To get a page that corresponds to one that we see in a browser when following a certain URL, one could use a **headless browser**. In short, headless browsers are web browsers without a graphical user interface and are usually controlled programmatically or via a command-line interface. They are used in several contexts but most notably for **automatic usability testing** and **web scraping**. In web extration we need a DOM representaion of the exact content as it is rendered in a full browser.\n",
    "\n",
    "To run a headless browser in Python one option is to use **Headless Chrome** together with **Selenium**. See [here](https://duo.com/decipher/driving-headless-chrome-with-python) for instructions of how to install and use Headless Chrome with Python.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text\n",
    "This simple code shows how to extract text parts from a web page. It uses <code>beautifulsoap</code> library which cleans up <code>HTTP request payload</code> and creates a DOM tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of code which extracts all text from a web page \n",
    "url_address = \"http://www.times.si\"\n",
    "\n",
    "with urllib.request.urlopen(url_address) as url:\n",
    "    html = url.read()\n",
    "    \n",
    "#html = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# kill all script and style elements\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    # rip it out\n",
    "\n",
    "# get text\n",
    "text = soup.get_text()\n",
    "\n",
    "# break into lines and remove leading and trailing space on each\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "# drop blank lines\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing text with NLTK\n",
    "NLTK stands for **Natural Language Processing Toolkit**. It is a Python library that supports numerous tasks that are common in NLP, such as *tokenization*, *lemmatization*, *stemming*, *pos tagging*, *semantic reasoning*, *parsing*, etc. It provides interfaces to many corpora and lexical resources such as *WordNet*.\n",
    "\n",
    "Below are few examples of how to use NLTK. For complete documentation see [NLTK webpage](https://www.nltk.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Tokenization\n",
    "Suggested reading: [The art of tokenization](https://www.ibm.com/developerworks/community/blogs/nlp/entry/tokenization?lang=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple tokenization\n",
    "sentence = \"You can\\'t say you didn\\'t know this was wrong! And yet you did it. I want you to read these books.\"\n",
    "sentence = \"This book was written in 1998 by Dan Taylor. It is about information extraction from web sources.\"\n",
    "sentence += \" If you haven't read it yet then do so ASAP.\"\n",
    "sentence = sentence.lower()\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling contractions, e.g. don't --> do not\n",
    "\n",
    "# example of contractions dictionary - incomplete!\n",
    "contractions_dict = { \n",
    "\"can't\": \"cannot\",\n",
    "\"didn't\": \"did not\",\n",
    "\"don't\": \"do not\",\n",
    "\"isn't\": \"is not\",\n",
    "\"haven't\": \"have not\"\n",
    "}\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "     def replace(match):\n",
    "         return contractions_dict[match.group(0)]\n",
    "     return contractions_re.sub(replace, s)\n",
    "\n",
    "tokens = nltk.word_tokenize(expand_contractions(sentence))\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print stopwords for English\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Punctation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n",
    "words_no_punct = [w for w in filtered_words if nonPunct.match(w)]\n",
    "\n",
    "print(words_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Using explicit RE pattern in tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example shows how to use RegexpTokenizer\n",
    "# Function preprocess does all: makes the sentence lowercase, creates tokens, removes stopwords and punctation\n",
    "# and then puts tokens back to a sentence\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "sentence = \"You can\\'t say you didn\\'t know! I told you several times and you know that.\"\n",
    "print(preprocess(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. POS tagging\n",
    "Part-of-speech tagging (POS tagging) is grammatical analysis which marks each word in a text (corpus) as corresponding to a particular part of speech, e.g. <code>noun</code>, <code>verb</code>, <code>adjective</code>, <code>adverb</code>, etc. \n",
    "\n",
    "For details on *Categorizing and Tagging Wordshttp* check [here](www.nltk.org/book/ch05.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsPOS = nltk.pos_tag(words_no_punct)\n",
    "# I am creating this set for for the use in the lemmatizer\n",
    "wordsPOSset = {}\n",
    "for i in range (0, len(wordsPOS)):\n",
    "    wordsPOSset.update({(wordsPOS[i][0]):wordsPOS[i][1][0]})\n",
    "    print(wordsPOS[i][0] + \":\" + wordsPOS[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a simple stemming\n",
    "ps = PorterStemmer()\n",
    " \n",
    "for word in words_no_punct:\n",
    "    print(word + \":\" + ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example shows how to use NLTK lemmatizer. Remember that you have to tell what is the word you \n",
    "# would like to lemmatize. By default the lemmatizer expects a noun. But it could verb, adverb, adjecive and so on. \n",
    "\n",
    "# Pos tagger was trained on treebank corpora. This function maps treebank tags \n",
    "# into wordnet tags as expected in the lemmatizer.\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        #if not J, V, N or R then make it default, i.e. \"n\" as naun \n",
    "        return 'n'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in words_no_punct:\n",
    "    print(word + \":\" + lemmatizer.lemmatize(word, pos=get_wordnet_pos(wordsPOSset[word])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Working with Polyglot\n",
    "Polyglot is a library for natural language processing with support for **many languages** including **Slovene**. The following features are available:\n",
    "* Tokenization (165 Languages)\n",
    "* Language detection (196 Languages)\n",
    "* Named Entity Recognition (40 Languages)\n",
    "* Part of Speech Tagging (16 Languages)\n",
    "* Sentiment Analysis (136 Languages)\n",
    "* Word Embeddings (137 Languages)\n",
    "* Morphological analysis (135 Languages)\n",
    "* Transliteration (69 Languages)\n",
    "\n",
    "### 3.1. Installation\n",
    "Installation of <code>polyglot</code> library is a bit more complicated. To save you some time, I provide instructions that worked for me on <code>macOS Mojave ver. 10.14.3.</code> and <code>Python 3.7</code>.\n",
    "\n",
    "    # Install icu4c\n",
    "    $ brew install icu4c\n",
    "# Check icu4c version … in my case 63.1\n",
    "$ ls /usr/local/Cellar/icu4c/\n",
    "    # link icu-config to standard path\n",
    "    $ ln -s /usr/local/Cellar/icu4c/63.1/bin/icu-config /usr/local/bin/icu-config \n",
    "# install pyicu and other dependencies for polyglot\n",
    "$ sudo pip3 install pyicu\n",
    "    $ pip3 install pycld2\n",
    "$ pip3 install morfessor\n",
    "    $ pip3 install polyglot\n",
    "\n",
    "Below is an examples of how to use Polyglot for *language detection*. For more details check [Polyglot webpage](https://pypi.org/project/polyglot/).\n",
    "\n",
    "### 3.2. Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word\n",
    "\n",
    "text1 = Text(\"Danes je lep sončen dan.\")\n",
    "text2 = Text(\"Heute ist ein schöner sonniger Tag.\")\n",
    "text3 = Text(\"Today is a beautiful sunny day.\")\n",
    "print(\"Language Detected: Code={}, Name={}\\n\".format(text1.language.code, text1.language.name))\n",
    "print(\"Language Detected: Code={}, Name={}\\n\".format(text2.language.code, text2.language.name))\n",
    "print(\"Language Detected: Code={}, Name={}\\n\".format(text3.language.code, text3.language.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
