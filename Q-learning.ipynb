{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Reinforcement Learning to solve a Smartcab problem\n",
    "---\n",
    "**Author**: Marko Bajec\n",
    "\n",
    "**Last update**: 12.3.2019\n",
    "\n",
    "**Description**: this tutorial demonstrates how to use reinforcement learning (RL) for solving a problem of autonomous taxi car (Smartcab). The goal of the Smartcab is to pick up a passenger from a start location and drive him to the final location. The position of the Smartcab and start/final locations are all predefined for each taxi job. The algorithm has to learn three things: a) how to pick up a passenger (the taxi has to be at the location of the passenger), b) how to drive the passenger to the final location by using an optimal route, and c) how to drop off the passenger (again, this is only possible if the taxi is at the final location and the passenger is in the car). \n",
    "\n",
    "The tutorial is based on [Reinforcement Q-Learning from Scratch in Python with OpenAI Gym](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/).\n",
    "\n",
    "For gym documentation see [here](https://gym.openai.com/docs/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the Smartcab problem\n",
    "\n",
    "In RL, the problem is modeled by specifying **rewards**, **states**, and **actions**.\n",
    "\n",
    "### Rewards\n",
    "Since the agent (the imaginary driver) is reward-motivated and is going to learn how to control the cab by trial experiences in the environment, we need to decide the rewards and/or penalties and their magnitude accordingly. Here are a few points to consider:\n",
    "\n",
    "* The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired, let's say <code>20</code>,\n",
    "* The agent should be penalized if it tries to drop off a passenger in wrong locations, let it be <code>-10</code>\n",
    "* The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible, e.g. <code>-1</code>.\n",
    "\n",
    "### State Space\n",
    "In RL, the agent encounters a state, and then takes action according to the state it's in. The **State Space** is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.\n",
    "\n",
    "Let's say we have a training area for our Smartcab where we are teaching it to transport people in a parking lot to four different locations (<code>R</code>, <code>G</code>, <code>Y</code>, <code>B</code>):\n",
    "\n",
    "![Representation of the taxi problem environment](https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png).\n",
    "\n",
    "### Action Space\n",
    "The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger. In other words, we have six possible actions:\n",
    "* <code>south</code>,\n",
    "* <code>north</code>,\n",
    "* <code>east</code>,\n",
    "* <code>west</code>,\n",
    "* <code>pickup</code>,\n",
    "* <code>dropoff</code>.\n",
    "\n",
    "You'll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment's code, we will simply provide a <code>-1</code> penalty for every wall hit and the taxi won't move anywhere. This will just rack up penalties causing the taxi to consider going around the wall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the environment in Python\n",
    "To setup the environment, we import *gym* library and load \"Taxi-v2\" *environment*. This environment has already been prepared. It consists of:\n",
    "* 500 states (5x5 possible taxi positions, 5 positions of a passanger - including one in taxi, and 4 posible drop off locations. Altogether 5 x 5 x 5 x 4 = 500),\n",
    "* 6 actions (west, east, north, south, drop off, pick up).\n",
    "\n",
    "Note: if you haven't done so yet, install the *gym* library using <code>!pip3 install cmake 'gym[atari]' scipy</code>. Then try to import the library and render the space.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To setup a particular situation, environment method <code>encode</code> can be used. Remember that our state is described with four elements:\n",
    "* taxi x position,\n",
    "* taxi y position,\n",
    "* passanger index, and\n",
    "* destination index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Reward Table\n",
    "When the Taxi environment is created, there is an initial **Reward table** that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a $states × actions$ matrix.\n",
    "\n",
    "<p>To output the rewards for a particluar state and its actions, use <code>env.P[state]</code>.</p> \n",
    "<p>Each state, action pair is described with the following structure: <code>action: [(probability, nextstate, reward, done)].</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "* The <code>0-5</code> corresponds to the actions (*south*, *north*, *east*, *west*, *pickup*, *dropoff*) the taxi can perform at our current state in the illustration.\n",
    "* In this env, <code>probability</code> is always 1.0.\n",
    "* The <code>nextstate</code> is the state we would be in if we take the action at this index of the dict\n",
    "* All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
    "* <code>done</code> is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode.\n",
    "\n",
    "<p>Note that if our agent chose to explore action three (3) in this state it would be going West into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing -1 penalties (staying at the same position), which affects the **long-term reward**.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the taxi problem using brute force  \n",
    "We can try to solve the taxi problem by randomly seleting actions. We finish when the passanger is picked up and droped off at the final destination. It is clear from the example, that such an approach is useless - we make many wrong steps and receive a lot of penalties.  \n",
    "\n",
    "To randomly select an action, the method <code>sample</code> is used: <code>action = env.action_space.sample()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the environment\n",
    "To see the simulation of how the system was choosing actions, we just print the dictionary of frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that our agent takes thousands of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination. This is because we aren't *learning* from past experience. We can run this over and over, and it will never optimize. The agent has no memory of which action was best for each state, which is exactly what **Reinforcement Learning** will do for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Reinforcement Learning to solve the taxi problem\n",
    "\n",
    "We are going to use a simple RL algorithm called **Q-learning** which will give our agent some memory.\n",
    "\n",
    "### Intro to Q-learning\n",
    "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "\n",
    "In our Taxi environment, the reward table, <code>P</code> can help us to learn from past experience. This is done by taking an action in the current state and then updating a **Q-value** to remember if that action was beneficial. Q-values are remembered/updated every time an action is made in a particular state. Notice that Q-values pertain to a <code>state, action</code> pairs rather than to <code>states</code>. They are stored in a **Q-table**. \n",
    "\n",
    "A Q-value for a particular <code>state-action</code> combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.\n",
    "\n",
    "For example, if the taxi is faced with a state that includes a passenger at its current location, it is highly likely that the Q-value for <code>pickup</code> is higher when compared to other actions, like <code>dropoff</code> or <code>north</code>.\n",
    "\n",
    "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the following equation:\n",
    "\n",
    "$$Q(state,action)←(1−α)*Q(state,action)+α*(reward + γ*\\underset{a}{\\operatorname{max}}Q(next state,a))$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $α$ (alpha) is the learning rate $(0<α≤1)$. It tells the extent to which our Q-values are being updated in every iteration.\n",
    "- $γ$ (gamma) is the discount factor $(0≤γ≤1)$. It determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it $greedy$.\n",
    "\n",
    "### Exploiting learned values\n",
    "\n",
    "After enough random exploration of actions, the Q-values tend to **converge** serving our agent as an action-value function which it can **exploit** to pick the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between **exploration** (choosing a random action) and **exploitation** (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called $ϵ$ \"epsilon\" to cater to this during training.\n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in **episodes** with more penalties (on average) which is obvious because we are exploring and making random decisions.\n",
    "\n",
    "### Training the agent\n",
    "\n",
    "First, we'll initialize the Q-table to a 500×6 matrix of zeros:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the training algorithm that will update the <code>Q-table</code> as the agent explores the environment over thousands of episodes.\n",
    "\n",
    "In the first part of while not done, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the **epsilon value** and comparing it to the <code>random.uniform(0, 1)</code> function, which returns an arbitrary number between 0 and 1.\n",
    "\n",
    "We execute the chosen action in the environment to obtain the <code>next_state</code> and the <code>reward</code> from performing the action. After that, we calculate the <code>maximum Q-value</code> for the actions corresponding to the <code>next_state</code>, and with that, we can easily update our Q-value to the <code>new_q_value</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Q-table has been established over 100,000 episodes, let's see what the Q-values are at our illustration's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Q-table tells the rewords for each of the possible actions (*west*, *east*, *north*, *south*, *drop off*, and *pick up*). The max Q-value is for \"east\", so it looks like Q-learning has effectively learned the best action to take in our illustration's state!\n",
    "\n",
    "### Evaluation\n",
    "Now let's try how good is our model (Q-table). We will run 100 episodes and observe how many mistakes the Smartcab will do (remember that for each wrong move, i.e. wrong pick up, wrong drop off or wrong move - move to a wall, we get a penalty). What we expect is that since we use the learned model, the penalties should only rarely occur if at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
