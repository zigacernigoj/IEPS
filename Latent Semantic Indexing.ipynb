{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Indexing\n",
    "\n",
    "---\n",
    "**Author**: Marko Bajec\n",
    "\n",
    "**Last update**: 4.5.2019\n",
    "\n",
    "**Description**: in this example we show how to use **Singular Value Decomposition (SVD)** to transform terms and documents of a document corpus to a *k-concept space* and use the transformed vector space for Information Retrieval.  \n",
    "**Required libraries** (use pip3):\n",
    "* <code>numpy</code>\n",
    "* <code>scipy</code>\n",
    "* <code>sklearn</code>\n",
    "* <code>lemmagen</code>\n",
    "* <code>nltk</code>\n",
    "* <code>matplotlib</code>\n",
    "\n",
    "---\n",
    "## Document corpus\n",
    "Let's say we have the following 5 sentences representing the **document corpus**:\n",
    "* d<sub>1</sub>: *Romeo and Juliet.*\n",
    "* d<sub>2</sub>: *Juliet: O happy dagger!*\n",
    "* d<sub>3</sub>: *Romeo died by dagger.*\n",
    "* d<sub>4</sub>: *\"Live free or die”, that’s the New-Hampshire’s motto.*\n",
    "* d<sub>5</sub>: *Did you know, New-Hampshire is in New-England.*\n",
    "\n",
    "**Question**: How close (relevant) are the above sentences to the following query: $q=$<span style=\"color:blue\">*died, dagger*</span>?\n",
    "\n",
    "Remember that we used the same corpus for the exercise on the *Vector Space Models*. There the best matching documents for our query were $d_3$ and $d_2$. The document $d_1$ was however ranked very low despite the fact that in the play *Romeo and Juliet*, written by William Shakespeare, both Romeo and Juliet die by dagger. The problem is that $d_1$ doesn't involve any of the query words. We expect that by using **LSI** and **SVD**, documents including any of these words will come closer based on the fact that they often cooccur together. This should then result in getting the document $d_1$ closer to our query.\n",
    "\n",
    "\n",
    "## Python implementation\n",
    "In the Python program below we use <code>scipy</code> implementation of SVD to decompose our matrix to $A=U \\Sigma V^T$. Since our term-document matrix is small, i.e. $6 \\times 11$, we could have operated with the whole matrix (ignoring that in this way we would not get rid off any noise). Instead, we will use two dimensions only (i.e. $k=2$) just to demonstrate a complete SVD procedure including *reduction* of matrix dimensions. \n",
    "\n",
    "To compare the query and documents in the *k-concept space model*, we will use **Cosine distance** implemented in <code>sklearn</code>. The same library will also be used to form the **tf matrix**.   \n",
    "\n",
    "### Step 1: preprocess Corpus\n",
    "In this step we import the required libraries, define two supporting functions (one for preprocessing text and the other for reducing the size of a two-dimensional matrix - we will need both functions later) and perform text processing on the corpus. More specifically, we remove *punctuation* and *stopwords*, put all in *lowercase* and *lemmatize*. At the end we print out the Corpus vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from scipy.linalg import svd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import lemmagen.lemmatizer\n",
    "from lemmagen.lemmatizer import Lemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set figure size for plots [width, height]\n",
    "plt.rcParams['figure.figsize'] = [12, 12]\n",
    "\n",
    "# This is a simple function that processes the corpus by applying the following transformations: \n",
    "# * transform to lowercase, \n",
    "# * remove punctuation,\n",
    "# * lemmatize\n",
    "# All of this transformations are optional. \n",
    "# After transformations are applied, a vocabulary is created by removing stopwords (optional) and duplicates\n",
    "# Preprocess works for Slovene or English. Use the parameter lang to setup the target language (slovene or english). \n",
    "# By default it is set to english. For removing SLO stopwords with nltk, you need a list of SLO stopwords. \n",
    "# The list is included in the CLARIN database. See here: https://www.clarin.si/repository/xmlui/handle/11356/1109 \n",
    "def preproces(corpus, rm_punctuation=None, lemmatize=None, rm_stopwords=None, lowercase=None, lang=None):\n",
    "    if rm_punctuation==None:\n",
    "        rm_punctuation=1\n",
    "    if lemmatize==None:\n",
    "        lemmatize=1\n",
    "    if rm_stopwords==None:\n",
    "        rm_stopwords=1\n",
    "    if lowercase==None:\n",
    "        lowercase=1    \n",
    "    if lang==None:\n",
    "        language='english'\n",
    "    # put to lowercase\n",
    "    if lowercase:\n",
    "        corpus = [s.lower() for s in corpus]\n",
    "    # remove punctation\n",
    "    if rm_punctuation:\n",
    "        punct = string.punctuation + '’' + '”'\n",
    "        corpus = [s.translate(str.maketrans('', '', punct)) for s in corpus]\n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "        lemmatized_corpus = []\n",
    "        if lang=='slovene':\n",
    "            lemmatizer = Lemmatizer(dictionary=lemmagen.DICTIONARY_SLOVENE)            \n",
    "        else:\n",
    "            lemmatizer = Lemmatizer(dictionary=lemmagen.DICTIONARY_ENGLISH)\n",
    "        for l in corpus:\n",
    "            lemmatized_word_list = []\n",
    "            for word in l.split():\n",
    "                lemmatized_word_list.append(lemmatizer.lemmatize(word))\n",
    "            lemmatized_corpus.append(' '.join(word for word in lemmatized_word_list))\n",
    "            corpus = lemmatized_corpus\n",
    "    # create vocabulary\n",
    "    vocab = []\n",
    "    for doc in corpus:\n",
    "        for word in doc.split():\n",
    "            vocab.append(word)\n",
    "    # remove stopwords\n",
    "    vocab = [word for word in vocab if word not in stopwords.words(lang)]\n",
    "\n",
    "    #remove duplicates\n",
    "    vocab = set(vocab)\n",
    "    \n",
    "    return corpus, vocab\n",
    "\n",
    "# reducematrix is a function that returns reduced two-dimensional matrix A to k elements according to the dimension dim.\n",
    "# dim can be 0 (meaning rows), 1 (meaning columns), or 2 (meaning rows and columns)\n",
    "def reduce_matrix(A, k, dim):\n",
    "    if dim not in {0, 1 ,2}:\n",
    "        return A\n",
    "    if dim == 2:\n",
    "        k_row = min(k, A.shape[0])\n",
    "        k_col = min(k, A.shape[1])\n",
    "        return A[0:k_row, 0:k_col]\n",
    "    k_dim = min(k, A.shape[dim])\n",
    "    if dim == 1:\n",
    "        return A[:, 0:k_dim]\n",
    "    if dim == 0:\n",
    "        return A[0:k_dim, :]\n",
    "    \n",
    "\n",
    "# document corpus\n",
    "corpus = [\"Romeo and Juliet.\", \n",
    "          \"Juliet: O happy dagger!\", \n",
    "          \"Romeo died by dagger.\", \n",
    "          \"'Live free or die'”, that’s the New-Hampshire’s motto.\", \n",
    "          \"Did you know, New-Hampshire is in New-England.\"\n",
    "         ]\n",
    "\n",
    "# query\n",
    "query = \"died, dagger\"\n",
    "\n",
    "# process the corpus by removing punctuation, stopwords, change to lowercase, create vocabulary\n",
    "corpus_processed, corpus_vocabulary = preproces(corpus, rm_punctuation=True, \n",
    "                                                        lemmatize=True, \n",
    "                                                        lowercase=True, \n",
    "                                                        rm_stopwords=True,\n",
    "                                                        lang='english') \n",
    "\n",
    "print(\" \")\n",
    "print('Corpus vocabulary')\n",
    "print(\"=======================================================================================\")\n",
    "print(corpus_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: create TF matrix\n",
    "In the second step we create *term-frequency matrix* for the Corpus. For this purpose we use Count Vectorizer that comes with <code>sklearn</code> library. As a result, we print out the feature names (terms from our vocabulary in the order as they appear in the TF matrix), dimensions of the TF matrix and its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create term-frequency matrix\n",
    "cv =  CountVectorizer(vocabulary = corpus_vocabulary)\n",
    "tf_array = cv.fit_transform(corpus_processed).transpose()\n",
    "\n",
    "#print(print feature names and TF matrix)\n",
    "print(\" \")\n",
    "print('Feature names and TF matrix')\n",
    "print(\"=======================================================================================\")\n",
    "corpus_vocabulary = cv.get_feature_names()\n",
    "print(cv.get_feature_names())\n",
    "print(\" \")\n",
    "print(tf_array.get_shape())\n",
    "print(tf_array.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: transform TF matrix with SVD\n",
    "In this step we use SVD from <code>scipy</code> library to get the required matrix decomposition. Than we reduce the matrices to the size k and print out the term representations, document representations and singular values, all for the **k-concept space**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the term-frequency matrix with SVD \n",
    "# note that by default CountVectorizer use sparse matrices - that's why we need its dense version \n",
    "\n",
    "U, s, VT = svd(tf_array.todense())\n",
    "Uk = reduce_matrix(U, 2, 1)\n",
    "VTk = reduce_matrix(VT, 2, 0)\n",
    "sk = reduce_matrix(np.diag(s), 2, 2)\n",
    "\n",
    "# print term vectors in k-concept space (i.e. row vectors of U)\n",
    "print(\" \")\n",
    "print('Term representations')\n",
    "print(\"=======================================================================================\")\n",
    "for i in range(0, len(corpus_vocabulary)):\n",
    "    print('{:15s} {:7.4f}  {:7.4f}'.format( corpus_vocabulary[i], Uk[i][0], Uk[i][1] )) \n",
    "\n",
    "# print document vectors in k-concept space (i.e. column vectors of VT)\n",
    "print(\" \")\n",
    "print('Documents representations')\n",
    "print(\"=======================================================================================\")\n",
    "for i in range(0, len(corpus)):\n",
    "    print('{:45s} {:7.4f}  {:7.4f}'.format( corpus_processed[i], VTk[0][i], VTk[1][i] )) \n",
    "    \n",
    "# print singular values\n",
    "print(\" \")\n",
    "print('Reduced singular values')\n",
    "print(\"=======================================================================================\")\n",
    "print(sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: calculate query representation in the k-concept space\n",
    "In step 4, we transform the query so that it is represented as another document in the k-concept space. Its representation is then printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate query representation in the k-concept space\n",
    "# first pre-process the original query\n",
    "query_processed, vocab_query = preproces([query])\n",
    "# use the same Count Vectorizer to get the TF representation of q \n",
    "qT = cv.fit_transform(query_processed).todense().tolist()[0]\n",
    "# calculate inversed sk matrix\n",
    "sk_inv = np.linalg.inv(sk)\n",
    "# multiply qT, Uk and sk_inv to get qk\n",
    "qk = np.matmul(np.matmul(qT, Uk), sk_inv)\n",
    "\n",
    "# print qk\n",
    "print(\" \")\n",
    "print('Query representation in the k-concept space')\n",
    "print(\"=======================================================================================\")\n",
    "print(qk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: calculate Cosine similarity \n",
    "For each document in the Corpus, we calculate and print out its *Cosine similarity* to the query. For the similarity calculation we use the implementation available in the <code>sklearn</code> library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each document as represented in VTk, calculate cosine distance to qk and print it\n",
    "print(\" \")\n",
    "print('Cosine similarity (di, q)')\n",
    "print(\"=======================================================================================\")\n",
    "for i in range (0, len(corpus)):\n",
    "    print('{:5s} {:7.4f}'.format('d'+str(i+1), cosine_similarity( [VTk[:,i]], [qk])[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: plot documents, query and terms in the k-concept space \n",
    "Finaly, we plot vectors representing documents, the query and the terms from our vocabulary in the k-concept space. We use the <code>matplotlib</code> library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot docs and the query in the k-concept space\n",
    "# draw axes\n",
    "ax = plt.axes()\n",
    "# draw docs\n",
    "for i in range (0, len(corpus)):\n",
    "    ax.arrow(0, 0, VTk[0,i], VTk[1,i])\n",
    "    plt.scatter(VTk[0,i], VTk[1,i], color='black')\n",
    "    ax.annotate('d'+str(i+1), (VTk[0,i]+0.02,VTk[1,i]),fontsize=14)\n",
    "\n",
    "# draw query\n",
    "ax.arrow(0, 0, qk[0], qk[1], color = 'red')\n",
    "plt.scatter(qk[0], qk[1], color='red')\n",
    "ax.annotate('q', (qk[0]+0.02,qk[1]),fontsize=14, color='red')\n",
    "\n",
    "# draw terms - only points\n",
    "for i in range (0, len(corpus_vocabulary)):\n",
    "    plt.scatter(Uk[i,0], Uk[i,1], color='blue')\n",
    "    ax.annotate(corpus_vocabulary[i], (Uk[i,0]+0.02,Uk[i,1]+0.0),fontsize=11, color='blue')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(-0.4,0.8)\n",
    "\n",
    "plt.title('Query and corpus documents in the k-concept space',fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Observe that our document $d_1$: *'Romeo and Juliet.'* got much higher rank with **Latent Semantic Indexing** than in our **vector space model**, where its Cosine similarity to the query was zero.  \n",
    "\n",
    "| Method | Dot product | Cosine similarity | Latent Semantic Indexing |\n",
    "| ------ | -----------:| -----------------:| -----:|\n",
    "| $d_1$  |           0 |                 0 | 0.8617|\n",
    "| $d_2$  |       0.196 |             0.221 | 0.8440|\n",
    "| $d_3$  |       0.392 |             0.666 | 0.9886|\n",
    "| $d_4$  |       0.152 |             0.146 | 0.4350|\n",
    "| $d_5$  |           0 |                 0 | 0.1486|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
